{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--require 'itorch'\n",
    "require 'torch'   -- torch\n",
    "require 'image'   -- for image transforms\n",
    "require 'nn'      -- provides all sorts of trainable modules/layers\n",
    "require 'InputDropout'\n",
    "require 'AtomLookupTable'\n",
    "\n",
    "   -- Simple 2-layer neural network, with tanh hidden units\n",
    "   nbr_input_size = 3\n",
    "   nbr_atom_types = 5\n",
    "   descriptor_length = 7\n",
    "  -- atom_representation = torch.Tensor(nbr_atom_types, descriptor_length)\n",
    "\n",
    "   model = nn.Sequential()\n",
    "   model:add(nn.AtomLookupTable(nbr_atom_types, descriptor_length))\n",
    "   model:add(nn.SplitTable(1))\n",
    "     p = nn.ParallelTable()\n",
    "     for i =1,nbr_input_size do\n",
    "       --p2 = nn.Linear(7,3)\n",
    "       p2 = nn.Sequential()\n",
    "       p2:add(nn.SplitTable(1))\n",
    "       p2:add(nn.DotProduct())\n",
    "       p:add(p2)\n",
    "     end\n",
    "   model:add(p)\n",
    "   model:add(nn.JoinTable(1))\n",
    "   scaling_layer = nn.CMul(nbr_input_size)\n",
    "   tempN = scaling_layer.weight:size()\n",
    "   scaling_layer.weight = torch.Tensor(tempN):fill(0.5)\n",
    "   -- -model:get(1).weight = a -- torch.Tensor(2,1,1,3)\n",
    "   -- add scaling of output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userdata\t\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "/usr/local/share/lua/5.1/nn/Linear.lua:71: expected arguments: *DoubleTensor~1D* [DoubleTensor~1D] [double] DoubleTensor~2D DoubleTensor~1D | *DoubleTensor~1D* double [DoubleTensor~1D] double DoubleTensor~2D DoubleTensor~1D\nstack traceback:\n\t[C]: in function 'addmv'\n\t/usr/local/share/lua/5.1/nn/Linear.lua:71: in function 'updateGradInput'\n\t/usr/local/share/lua/5.1/nn/Sequential.lua:36: in function 'updateGradInput'\n\t/usr/local/share/lua/5.1/nn/Module.lua:30: in function 'backward'\n\t[string \"...\"]:21: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/viviana/torch/install/share/lua/5.1/itorch/main.lua:174: in function </Users/viviana/torch/install/share/lua/5.1/itorch/main.lua:140>\n\t/Users/viviana/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...s/viviana/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...s/viviana/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...s/viviana/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/viviana/torch/install/share/lua/5.1/itorch/main.lua:341: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010491f7a0",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "/usr/local/share/lua/5.1/nn/Linear.lua:71: expected arguments: *DoubleTensor~1D* [DoubleTensor~1D] [double] DoubleTensor~2D DoubleTensor~1D | *DoubleTensor~1D* double [DoubleTensor~1D] double DoubleTensor~2D DoubleTensor~1D\nstack traceback:\n\t[C]: in function 'addmv'\n\t/usr/local/share/lua/5.1/nn/Linear.lua:71: in function 'updateGradInput'\n\t/usr/local/share/lua/5.1/nn/Sequential.lua:36: in function 'updateGradInput'\n\t/usr/local/share/lua/5.1/nn/Module.lua:30: in function 'backward'\n\t[string \"...\"]:21: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/viviana/torch/install/share/lua/5.1/itorch/main.lua:174: in function </Users/viviana/torch/install/share/lua/5.1/itorch/main.lua:140>\n\t/Users/viviana/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...s/viviana/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...s/viviana/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...s/viviana/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/viviana/torch/install/share/lua/5.1/itorch/main.lua:341: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010491f7a0"
     ]
    }
   ],
   "source": [
    "\n",
    "    model:add(scaling_layer)\n",
    "   model:add(nn.Tanh())\n",
    "   model:add(nn.Linear(nbr_input_size, 1))\n",
    "   input = torch.Tensor({{1,2},{3,4},{5,2}}) --,  {{5,5},{5,5},{5,2}} })\n",
    "   inputtable = {}\n",
    "   inputtable[1] = input\n",
    "   inputtable[2] = input\n",
    "   criterion = nn.MSECriterion()\n",
    "   criterion.sizeAverage = false\n",
    "   print(type(input))\n",
    "\n",
    "   model:training()\n",
    "   parameters, gradParameters = model:getParameters()\n",
    "   gradParameters:zero()\n",
    "   target = torch.Tensor({100})\n",
    "   loutput = model:forward(inputtable[1])\n",
    "   lerr = criterion:forward(loutput, target)\n",
    "   ldf_fo = criterion:backward(loutput, target)\n",
    "\n",
    "   model:backward(inputtable[1], ldf_do)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
